---
title: "Intro to ML, 2nd Half Problems"
output:
  pdf_document: default
  html_document: default
group: Garrett Soter, AJ Cooper, Trevor Moos, Ben Sullivan
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Visual story telling part1: green buildings

We first began analysis by creating various scatter plots. This was done to get a good first look at any strong relationships that could be found in the data. 


```{r include = FALSE}
library(mosaic)
green = read.csv('greenbuildings.csv')
green_only = subset(green, green_rating==1)
non_green = subset(green, green_rating!=1)
a_only = subset(green, class_a==1)
b_only = subset(green, class_b==1)
green_onlya = subset(a_only, green_rating==1)
non_greena = subset(a_only, green_rating!=1)
green_onlyb = subset(b_only, green_rating==1)
non_greenb = subset(b_only, green_rating!=1)
```

```{r include = FALSE}
summary(green)
```

Below you can see that there seems to be a trend between leasing rates and the amount of rent charged. This seemed to be at the very least correlated. We decided to keep this in the data when moving forward with out analysis

```{r echo = FALSE}
plot(green$leasing_rate, green$Rent)
```

```{r echo = FALSE}
ggplot(green_only) + 
  geom_point(aes(x=cluster_rent, y=Rent, color = class_a))+
  labs(x = 'Cluster Rent ($ per sq ft)', y = 'Green Buliding Rent ($ per sq ft)', title = 'Green Building Rent vs Associated Cluseter Rent')

```

We then Boot strapped the data to normalize the means for future analysis

```{r include = FALSE}
boot5 = do(2500)*{
  mean(resample(a_only)$Rent)
}
boot6 = do(2500)*{
  mean(resample(b_only)$Rent)
}
boot1 = do(2500)*{
  mean(resample(green_onlya)$Rent)
}
boot2 = do(2500)*{
  mean(resample(non_greena)$Rent)
}
boot3 = do(2500)*{
  mean(resample(green_onlyb)$Rent)
}
boot4 = do(2500)*{
  mean(resample(non_greenb)$Rent)
}
```

We then found the proportion of green buildings to normal buildings in group 'a' and then for group 'b'. We hypothesisesd that the seemingly higher rate for the rent in the green bulidings was because there were more in A and not becaus the were inheirently more valuable.

```{r echo = FALSE}
nrow(green_onlya)/nrow(a_only)*100
print('For A')
nrow(green_onlyb)/nrow(b_only)*100
print('For b')
```

17.3% of buildings in A were green where as only 3.6% in B

```{r echo = FALSE}
hist(boot6$result, 45, col = 'pink', xlab = 'Rent($ per sqft)', main = 'Prices of the mean rent in both places', xlim=c(25,35))
hist(boot5$result, 45, col = 'purple', xlab = 'Rent($ per sqft)', add = T)
  legend("right", c("Area B", "Area A"), fill=c("pink", "purple"))
```

You can clearly see a price difference in the mean prices, the valley between the histograms of boot strapped mean process clearly shows this is a significant split

```{r echo = FALSE}
hist(boot1$result, 45, col = 'green', xlab = 'Rent($ per sqft)', main = 'Green bulidings and non green buildings in the prime location')
hist(boot2$result, 45, col = 'blue', add = T)
  legend("right", c("Green", "Non_green"), fill=c("green", "blue"))
```

This actually demonstrates a point counter to the hypothesis of the 'Excel Guru', when we contorl of the location the prime location green bulidings actually rent out at a discount.

```{r echo = FALSE}
hist(boot3$result, 30, col = 'green', xlab = 'Rent($ per sqft)', main = 'Green bulidings and non green buildings in the lesser location')
hist(boot4$result, 30, col = 'blue', add = T)
  legend("right", c("Green", "Non_green"), fill=c("green", "blue"))
```

Same as in the B side of town, you can see a higher mean housing price for the non_green houses, this also runs counter to the analysis of the in house exel guru.

```{r echo = FALSE}
hist(non_greena$Rent, xlab = 'Mean Rent for location A', ylab = 'Frequency', col = 'blue', main = 'Relative frequency of green to non greeen buildings in group A', breaks = 25)
hist(green_onlya$Rent, xlab = 'Mean Rent for location A', ylab = 'Frequency', col = 'green', breaks = 25, add = T)
  legend("right", c("Green", "Non_green"), fill=c("green", "blue"))
```

```{r echo = FALSE}
hist(non_greenb$Rent, xlab = 'Mean Rent for location B', ylab = 'Frequency', col = 'blue', main = 'Relative frequency of green to non greeen buildings in group B', breaks = 25)
hist(green_onlyb$Rent, xlab = 'Mean Rent for location B', ylab = 'Frequency', col = 'green',  breaks = 25, add = T)
  legend("right", c("Green", "Non_green"), fill=c("green", "blue"))
```

It is clearly visible that there are much less green houses in the B part of town, driving the aggragate green price up, dispite it being cheaper than comprabible houses, as they say in the business LOCATION LOCATION LOCATION.
##Visulization 2

```{r}
#loading of the data and converting the categorical columns into factors
library(ggplot2)
flight_data=read.csv('ABIA.csv')
flight_data=flight_data[-1]
#str(flight_data)
flight_data$UniqueCarrier=as.factor(flight_data$UniqueCarrier)
no_divert_no_cancel_fd=flight_data[(flight_data$Diverted==0 & flight_data$CancellationCode==''),]
```



```{r}
#plotting distributions of flights per month, day of month, day of week, and unique carriers
#month is mostly uniform, though flights peak in th summer and tended to tail by the end of the year
ggplot(flight_data, aes(Month)) +
  geom_bar(fill = "#0073C2FF")
#day of month is irrelevant to flight volume
ggplot(flight_data, aes(DayofMonth)) +
  geom_bar(fill = "#0073C2FF")
#week days were fairly even, saturday's were the least flown day
ggplot(flight_data, aes(DayOfWeek)) +
  geom_bar(fill = "#0073C2FF")
#WN is southwest which is the overwhelming majority of flights to and from AIBA
ggplot(flight_data, aes(UniqueCarrier)) +
  geom_bar(fill = "#0073C2FF")
```

```{r Simple Delay Histograms}
#Here are some histograms of all the different delays, but we will focus mostly on arrival delays
attach(flight_data)
hist(x=ArrDelay, col="lightblue", main="Histogram of Arrival Delays", 
     xlab="Arrival Delays") 
hist(x=DepDelay, col="lightblue", main="Histogram of Departure Delays", 
     xlab="Departure Delays")
hist(x=CarrierDelay, col="lightblue", main="Histogram of Carrier Delays", 
     xlab="Carrier Delays")
hist(x=WeatherDelay, col="lightblue", main="Histogram of Weather Delays", 
     xlab="Weather Delays")
hist(x=NASDelay, col="lightblue", main="Histogram of NAS Delays", 
     xlab="NAS Delays")
hist(x=SecurityDelay, col="lightblue", main="Histogram of Security Delays", 
     xlab="Security Delays")
hist(x=LateAircraftDelay, col="lightblue", main="Histogram of Late Aircraft Delays", 
     xlab="Late Aircraft Delays")
```


```{r Delays by Month}
# Arrival Delays frequency per month (count of any time delay)
ArrMonth <- ggplot(data=flight_data, aes(x=Month, y=ArrDelay)) +
  geom_bar(stat="identity", position='dodge', fill="steelblue") +
  ggtitle('Arrival Delays by Month') +
  ylab('Arrival Delays')
ArrMonth
# Arrival Delays frequency per day of the week. Interesting to note that saturday is so frequent, when as we see below, is shortest for the average delay
ArrDayOfWeek <- ggplot(data=flight_data, aes(x=DayOfWeek, y=ArrDelay)) +
  geom_bar(stat="identity", position='dodge', fill="steelblue") +
  ggtitle('Arrival Delays by Day of Week') +
  ylab('Arrival Delays')
ArrDayOfWeek
# Arrival Delays plotted over time of day. Delays appear to mount up as the day goes on.
ArrDayOfWeek <- ggplot(data=flight_data, aes(x=DepTime, y=ArrDelay)) +
  geom_point(col="red", size=1) +
  ggtitle('Arrival Delays by Time of Day') +
  ylab('Arrival Delays')
ArrDayOfWeek
```

```{r}
#plot of the distribution of delays, overall median delay time is about the same for almost all of the different carriers.
plot(no_divert_no_cancel_fd$UniqueCarrier, no_divert_no_cancel_fd$ArrDelay, labels=no_divert_no_cancel_fd$UniqueCarrier, cex.axis=0.9)
```
```{r}
#calculating number of delayed flights per carrier
wn_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'WN',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'WN',])
nineE_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == '9E',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == '9E',])
aa_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'AA',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'AA',])
b6_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'B6',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'B6',])
co_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'CO',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'CO',])
dl_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'DL',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'DL',])
ev_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'EV',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'EV',])
f9_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'F9',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'F9',])
mq_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'MQ',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'MQ',])
nw_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'NW',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'NW',])
oh_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'OH',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'OH',])
oo_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'OO',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'UA',])
ua_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'UA',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'UA',])
us_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'US',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'US',])
xe_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'XE',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'XE',])
yv_delay_prop=nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$ArrDelay > 0 & no_divert_no_cancel_fd$UniqueCarrier == 'YV',])/nrow(no_divert_no_cancel_fd[no_divert_no_cancel_fd$UniqueCarrier == 'YV',])
```


```{r}
delay_prop_vec=c(nineE_delay_prop, aa_delay_prop, b6_delay_prop, co_delay_prop, dl_delay_prop, ev_delay_prop, f9_delay_prop, mq_delay_prop, nw_delay_prop, oh_delay_prop, oo_delay_prop, ua_delay_prop, us_delay_prop, wn_delay_prop, xe_delay_prop, yv_delay_prop)
#plot of the percentage of flights which are delayed per carrier, 94% of OO's flights are delayed, otherwise mostly uniform
barplot(delay_prop_vec, names.arg=sort(unique(no_divert_no_cancel_fd$UniqueCarrier),decreasing=FALSE))
```

```{r}
#plots of arrival delay in relation to the distance the plane has to travel, no correlation
plot(no_divert_no_cancel_fd$Distance, no_divert_no_cancel_fd$ArrDelay, type='p')
cor(no_divert_no_cancel_fd$Distance, no_divert_no_cancel_fd$ArrDelay)
```



```{r}
#converting more columns to factors for these graphs
no_divert_no_cancel_fd$Origin=as.factor(no_divert_no_cancel_fd$Origin)
no_divert_no_cancel_fd$Dest=as.factor(no_divert_no_cancel_fd$Dest)
#graoh of average flight delay based on flight origin
ggplot(data=no_divert_no_cancel_fd, aes(ArrDelay)) +
  stat_summary(aes(y = Origin), fun = "mean", geom = "bar", orientation = 'y') +
  theme(axis.text.y = element_text(size = 6))  
#graph of average flight delay based on flight destination
ggplot(data=no_divert_no_cancel_fd, aes(ArrDelay)) +
  stat_summary(aes(y = Dest), fun = "mean", geom = "bar", orientation = 'y') +
  theme(axis.text.y = element_text(size = 6)) 
```


```{r}
#average arrival delay plotted agaist the month and day of the week
#september, october, and november all have very short delays, december, june, and march all have above average delays, the rest of the months are fairly uniform
ggplot(data=no_divert_no_cancel_fd, aes(ArrDelay)) +
  stat_summary(aes(y = Month), fun = "mean", geom = "bar", orientation = 'y') +
  theme(axis.text.y = element_text(size = 6)) 
#Saturday's are less flown days, so reduced delay makes sense, other deviations are not as explainable. It is interesting to see that saturdays have such a low average delay, since as shown above, they had the most delays overall.
ggplot(data=no_divert_no_cancel_fd, aes(ArrDelay)) +
  stat_summary(aes(y = DayOfWeek), fun = "mean", geom = "bar", orientation = 'y') +
  theme(axis.text.y = element_text(size = 6)) 
#checking september value was legit, it is
#mean(no_divert_no_cancel_fd[no_divert_no_cancel_fd$Month==9,]$ArrDelay)
```

Overall, we would like to avoid OO airlines, fly on Saturday's, and ideally in the month of September. Over summer and the holidays seem to be an abyss of arrival delays, especially if they were to coincide with a Friday departure.



##THE PORTFOLIO SIMULATION
```{r include=FALSE}
rm(list = ls())
library(mosaic)
library(quantmod)
library(foreach)
```

### First Portfolio
```{r include=FALSE}
### First Portfolio
portfolio_div = c('DBEF', 'DBA', 'DRN','GLD', 'TAN', 'GRID', 'SPY')
prices = getSymbols(portfolio_div, from = '2016-08-06')

for(ticker in portfolio_div){
  expr = paste0(ticker, 'a = adjustOHLC(', ticker, ')')
  eval(parse(text=expr))
}

all_returns = cbind(ClCl(DBEFa), ClCl(DBAa),ClCl(DRNa),ClCl(GLDa),ClCl(TANa),ClCl(GRIDa),ClCl(SPYa))
all_returns = as.matrix(na.omit(all_returns))
N = nrow(all_returns)
```
We created this first portfolio in the hopes of getting a safe portfolio with some access to rewards with in a Biden Harris clean energy administration
These are the stocks with in it:
Tan and Grid are sustainable - because biden politics
SPY is S&P index, heavily weighted towards this - tried and true
GLD is gold - hedges against crazy market movement
DBA is agraculture - you always need food
DBEF is a hedged equity fund - another amrket fund
DRN is realestate - moves with economy & am betting on bull market
```{r echo=FALSE}
wealth = 100000
simulation = foreach(i=1:5000, .combine = 'rbind') %do% {
  total_wealth = wealth
  my_weights = c(.1,.1,.1,.1,.1,.1,.4)
  holdings = total_wealth * my_weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days){
    return.today = resample(all_returns, 1, orig.ids = FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
    wealthtracker
}
head(simulation)
```
```{r echo = FALSE}
hist(simulation[,n_days], 25)

```
```{r echo=FALSE}
plot(wealthtracker, type = 'l')
```
The VAR for the safeish portfolio is this. Meaning you can be 95% confident this is the most you'll lose
```{r echo = FALSE}
quantile(simulation[,n_days]- wealth, prob=0.05)
```
### Second Portfolio
```{r include=FALSE}
portfolio_sus = c('FAN', 'CNRG', 'ERTH','ACES', 'TAN')

prices = getSymbols(portfolio_sus, from = '2016-08-06')

for(ticker in portfolio_sus){
  expr = paste0(ticker, 'a = adjustOHLC(', ticker, ')')
  eval(parse(text=expr))
}

all_returns = cbind(ClCl(FANa), ClCl(CNRGa),ClCl(ERTHa),ClCl(ACESa),ClCl(TANa))
all_returns = as.matrix(na.omit(all_returns))
N = nrow(all_returns)
```
all indexes are for clean energy, with the biden admin creating policies that are enviro-oriented these could work out
```{r echo=FALSE}
wealth = 100000
simulation = foreach(i=1:5000, .combine = 'rbind') %do% {
  total_wealth = wealth
  my_weights = c(.2,.2,.2,.2,.2)
  holdings = total_wealth * my_weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days){
    return.today = resample(all_returns, 1, orig.ids = FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}
head(simulation)
hist(simulation[,n_days], 25)

```

```{r echo = FALSE}
hist(simulation[,n_days], 25)
```
```{r echo=FALSE}
plot(wealthtracker, type = 'l')
```
The VAR for the green portfolio is this. Meaning you can be 95% confident this is the most you'll lose
```{r}
quantile(simulation[,n_days]- wealth, prob=0.05)
```

###Portfolio 3
```{r include=FALSE}
portfolio_tech = c('SOXX', 'SKYY', 'CIBR','HACK', 'CLOU', 'BLOK','FINX','XNTK')

prices = getSymbols(portfolio_tech, from = '2016-08-06')

for(ticker in portfolio_tech){
  expr = paste0(ticker, 'a = adjustOHLC(', ticker, ')')
  eval(parse(text=expr))
}

all_returns = cbind(ClCl(SOXXa), ClCl(SKYYa),ClCl(CIBRa),ClCl(HACKa),ClCl(CLOUa),ClCl(BLOKa),ClCl(FINXa),ClCl(XNTKa))
all_returns = as.matrix(na.omit(all_returns))
N = nrow(all_returns)
```
This is full of stocks that track tech indexes

```{r include=FALSE}
wealth = 100000
simulation = foreach(i=1:5000, .combine = 'rbind') %do% {
  total_wealth = wealth
  my_weights = c(.125,.125,.125,.125,.125)
  holdings = total_wealth * my_weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days){
    return.today = resample(all_returns, 1, orig.ids = FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}
head(simulation)
```

```{r echo = FALSE}
hist(simulation[,n_days], 25)
```

```{r include=FALSE}
simulation
```
```{r echo = FALSE}
hist(simulation[,n_days], 25)
plot(wealthtracker, type = 'l')
``` 
The VAR for the Tech portfolio is this. Meaning you can be 95% confident this is the most you'll lose
```{r echo=FALSE}
quantile(simulation[,n_days]- wealth, prob=0.05)
```


## Market Segmentation

First load in the data an take a look at what we're working with.

```{r echo = FALSE}
library(ggplot2)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)
library(cluster)
soc_mkt = read.csv('social_marketing.csv', header=TRUE)
head(soc_mkt)
```

There appears to be 37 columns that categorize the data. 3 of which do not appear to be useful, uncategorized, spam, and adult so we will not include them. They are most likely from bots. Looking at the data I think the results will best be interpreted into 5 clusters (or categories) of people. With 32 catergories of data and about 6 per cluster, less clusters would make the targeted markets too generic and more would lead weak relationships to the markets.

```{r echo = FALSE}
# Center and scale the data
X = soc_mkt[,-c(1:2,6,36:37)]
X = scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
#Run K means ++
# Using kmeans++ initialization
clust = kmeanspp(X, k=5, nstart=25)
clust$center[1,]*sigma + mu
clust$center[2,]*sigma + mu
clust$center[3,]*sigma + mu
clust$center[4,]*sigma + mu
clust$center[5,]*sigma + mu
# A few plots with cluster membership shown
# qplot is in the ggplot2 library
#Cluster: This cluster appears to be associated with younger people of age at or near a college student
qplot(online_gaming, college_uni, data=soc_mkt, color=factor(clust$cluster))
#Cluster: This clusters associated to fashion, beuaty and shopping interests, probably mostly women
qplot(fashion, beauty, data=soc_mkt, color=factor(clust$cluster))
#Cluster: This cluster is people who are very active and like out doors, fitness, and nutrition
qplot(outdoors, personal_fitness, data=soc_mkt, color=factor(clust$cluster))
#Cluster: This cluster appears to be very global associated to travel and current events, perhaps international minded
qplot(travel, current_events, data=soc_mkt, color=factor(clust$cluster))
#Cluster: Older, family oriented people strong relations to parenting and religion
qplot(religion, parenting, data=soc_mkt, color=factor(clust$cluster))
#sum the errors
clust$tot.withinss
clust$betweenss
# Which cars are in which clusters?
which(clust$cluster == 1)
which(clust$cluster == 2)
which(clust$cluster == 3)
which(clust$cluster == 3)
which(clust$cluster == 3)
```




After grouping NutrientH20's social media audience into 5 separate clusters, we have identified 5 new market segments for the company to target. The clustering allowed to look for different categories of tweets that may have appeared to relate to each, then plot these categories and identify their association with each cluster. The first cluster identified showed assimilation to online gaming and college universities. This segment of users appears to be younger in age, at or around that of a college student. The next segment is contained users with strong liking for fashion and beauty. They are likely to be primarily women who enjoy shopping. The third segment is people who are physically active and enjoy things related to the out doors, fitness, nutrition, and adventure. The fourth segment market appears to be very global, associated to travel and current events, perhaps internationally minded. Lastly we identified a market that is very family oriented, they are likely to be older and have children, they enjoy talking about their families and religion. These segment markets identified for specific users in NutrientH20's social media audience and they can now effectively target ads with both association to their product and its audiences interests.

# Author Attribution

## Set up

To begin, we imported several libraries to help with reading the data, creating a corpus, dimensionality reduction, creating a naive bayes algorithm, and generating random forests.

Libraries to set up include: tm, magrittr, e1071, caret, class, plyr, dplyr, and randomForest.

We also defined a readerPlain function to extract the data in the next step.
```{r Set Up}
library(tm) 
library(magrittr)
library(e1071)
library(caret)
library(class)
library(plyr)
library(dplyr)
library(randomForest)

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), id=fname, language='en') }
```

## Train Set Up

To set up the train data, we used Sys.glob to import the necessary data files. Next, two empty lists were generated to store data from the files. 

Next, we used a for loop to read through and extract all of the folders and authors in the dataset and store the essential data (such as the individual works of each author along with their name) in the empty lists we created.

Then we used the readerPlain function and lapply to clean the file names from our no longer empty lists.
```{r Train Set Up}
train = Sys.glob('ReutersC50/C50train/*')

list_of_work = NULL
train_labels = NULL

for(author in train) {
  authors = substring(author, first=30)
  work = Sys.glob(paste0(author, '/*.txt'))
  list_of_work = append(list_of_work, work)
  train_labels = append(train_labels, rep(authors, length(work)))
}

all_work_train = lapply(list_of_work, readerPlain) 
names(all_work_train) = list_of_work
names(all_work_train) = sub('.txt', '', names(all_work_train))
```

## Creating a Train Mining Corpus and Pre-Processing

We then created a text mining corpus for the compiled list of authors' work in the previous step. 

Using tm_map, we then began the pre-processing and tokenization. During this process, we converted the text to all lowercase, removed any numbers, removed all punctuation, removed any unnecessary spaces, and removed a few stopwords. 

We then created a DocumentTermMatrix for the train data using the finalized corpus and a weighting of weightTfIdf. From the train DTM, we removed any sparse terms not in the 95th percentile of words from all documents. This DTM was printed below to get a basic summary, then set as a matrix.
```{r Train Corpus and Pre-Processing}
train_corp = Corpus(VectorSource(all_work_train))

train_corp = tm_map(train_corp, content_transformer(tolower))
train_corp = tm_map(train_corp, content_transformer(removeNumbers))
train_corp = tm_map(train_corp, content_transformer(removePunctuation))
train_corp = tm_map(train_corp, content_transformer(stripWhitespace))
train_corp = tm_map(train_corp, content_transformer(removeWords), stopwords("SMART"))

train_DTM = DocumentTermMatrix(train_corp, list(weighting=weightTfIdf))
train_DTM = removeSparseTerms(train_DTM, 0.95)
train_DTM
train_DTM = as.matrix(train_DTM)
```

## Test Set Up

To set up the test data, we repeated steps from the train data. First, we used Sys.glob to import the necessary data files. Next, two empty lists were generated to store data from the files. 

We then used a for loop to read through and extract all of the folders and authors in the dataset and store the essential data (such as the individual works of each author along with their name) in the empty lists we created.

Then we used the readerPlain function and lapply to clean the file names from our no longer empty lists.
```{r Test Set Up}
test = Sys.glob('ReutersC50/C50test/*')

list_of_work = NULL
test_labels = NULL

for(author in test) {
  authors = substring(author, first=30)
  work = Sys.glob(paste0(author, '/*.txt'))
  list_of_work = append(list_of_work, work)
  test_labels = append(test_labels, rep(authors, length(work)))
}

all_work_test = lapply(list_of_work, readerPlain) 
names(all_work_test) = list_of_work
names(all_work_test) = sub('.txt', '', names(all_work_test))
```

## Creating a Test Mining Corpus and Pre-Processing

The corpus generation and pre-processing was also similar to previous steps. We created a text mining corpus for the compiled list of authors' work in the previous step. 

Using tm_map, we then began the pre-processing and tokenization. During this process, we converted the text to all lowercase, removed any numbers, removed all punctuation, removed any unnecessary spaces, and removed a few stopwords. 

We then created a DocumentTermMatrix for the train data using the finalized corpus and a weighting of weightTfIdf. From the train DTM, we removed any sparse terms not in the 95th percentile of words from all documents. The DTM also used specified a dictionary of column names from the train DTM. This DTM was printed below to get a basic summary, then set as a matrix.
```{r Test Corpus and Pre-Processing}
test_corp = Corpus(VectorSource(all_work_test))

test_corp = tm_map(test_corp, content_transformer(tolower))
test_corp = tm_map(test_corp, content_transformer(removeNumbers))
test_corp = tm_map(test_corp, content_transformer(removePunctuation))
test_corp = tm_map(test_corp, content_transformer(stripWhitespace))
test_corp = tm_map(test_corp, content_transformer(removeWords), stopwords("SMART"))

test_DTM = DocumentTermMatrix(test_corp, list(weighting=weightTfIdf, 
                                              dictionary=colnames(train_DTM)))
test_DTM = removeSparseTerms(test_DTM, 0.95)
test_DTM
test_DTM = as.matrix(test_DTM)
```

## Dimensionality Reduction

As part of the dimensionality reduction in order to build supervised models in later steps, we first cleaned the DTMs to exclude any 0 values and crosscheck elements in both DTMs.

We then used principal component analysis (prcomp) on the train set and created a new train class data frame that fits the size of the test DTM [,1:592]. We then did the same for the test DTM.
```{r Dimensionality Reduction}
train_DTM = train_DTM[,which(colSums(train_DTM) != 0)] 
test_DTM = test_DTM[,which(colSums(test_DTM) != 0)]

train_DTM = train_DTM[,intersect(colnames(test_DTM),colnames(train_DTM))]
test_DTM = test_DTM[,intersect(colnames(test_DTM),colnames(train_DTM))]

prin_comp = prcomp(train_DTM,scale=TRUE)

train_class = data.frame(prin_comp$x[,1:592])
test_class <- scale(test_DTM) %*% prin_comp$rotation[,1:592]
test_class <- as.data.frame(test_class)
```

## Naive Bayes

Classification tags were then created and set as a factor using the train and test labels created previously and added to the new train and test classes.

A naive bayes algorithm was developed using train class and train classifiers. Based on this algorithm, predictions for the test class were developed. A temporary dataframe was then created to find the number of correct predictions and discover and the accuracy of the model, which turned out to be around 30%.
```{r Naive Bayes Model}
# train_class['classifier'] = train_labels
# test_class['result'] = test_labels
# 
# classifiers = as.factor(train_class$classifier)
# results = as.factor(test_class$result)
# 
# author_nb = naiveBayes(classifiers~., data=train_class)
# nb_preds = predict(author_nb, test_class)
# 
# temp = as.data.frame(cbind(results,nb_preds))
# temp$tag = ifelse(temp$results == temp$nb_preds,1,0)
# sum(temp$tag)*100/nrow(temp)
```

## Random Forest

For our random forest models, a seed was set, and the rf algorithm was developed using the train class and classifiers. We tested several different number of trees and mtry values to hypertune our model, but the code takes a significant amount of time to process, so only our best model was used in the code below (ntree=500, mtry=8).

A temporary dataframe was then created to find the number of correct predictions and discover and the accuracy of the model, which turned out to be around 82.68%, making this our best performing model.
```{r Random Forest Model}

# set.seed(1)
# 
# author_rf = randomForest(classifiers~.,data=train_class, ntree=5, mtry=8,importance=TRUE)
# rf_preds = predict(author_rf,data=test_class)
# author_rf
# 
# temp = as.data.frame(cbind(results,rf_preds))
# temp$tag = ifelse(temp$results == temp$rf_preds,1,0)
# sum(temp$tag)*100/nrow(temp)
```


##Association Rules problem
```{r}
library(reshape2)
library(arules)
library(arulesViz)
```

```{r}
#loading and cleaning data which can be forced into a transaction type for the apriori algorithm
grocery_data=read.csv('groceries.txt', header = FALSE)
grocery_data$List=rownames(grocery_data)
grocery_data$List=as.integer(grocery_data$List)
grocery_melt=melt(grocery_data, id.vars='List')
grocery_melt=grocery_melt[,-2]
grocery_melt=grocery_melt[!(grocery_melt$value == ""), ]
```


```{r}
#converting the list into the transaction form for the apriori algorithm
grocery_melt$List=as.factor(grocery_melt$List)
grocery_lists=split(x=grocery_melt$value, f=grocery_melt$List)
grocery_transactions = as(grocery_lists, "transactions")
```

```{r}
#creating the association rules
grocery_rules=apriori(grocery_transactions, parameter=list(support=.005, confidence=.1, maxlen=10))
#list of all association rules
inspect(grocery_rules)
#graphs for some metrics, two-key plot shows short lists have higher support and low confidence while longer lists tend to be the opposite
plot(grocery_rules)
plot(grocery_rules, method='two-key plot')
#visualization of clusters of items bought together based on specific confidence and support cutoffs
sub1 = subset(grocery_rules, subset=confidence > 0.01 & support > 0.005)
summary(sub1)
plot(sub1, method='graph')
```


